@article{greenai-cacm20,
author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
title = {Green {AI}},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3381831},
doi = {10.1145/3381831},
abstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},
journal = {Commun. ACM},
pages = {54â€“63},
numpages = {10}
}

@article{mooreslaw1965,
  author={Moore, Gordon E.},
  journal={Electronics}, 
  title={Cramming more components onto integrated circuits}, 
  year={1965},
  volume={38},
  number={8},
}

@article{dennardscaling1974,
  author={Dennard, R.H. and Gaensslen, F.H. and Yu, Hwa-Nien and Rideout, V.L. and Bassous, E. and LeBlanc, A.R.},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={Design of ion-implanted {MOSFET}'s with very small physical dimensions}, 
  year={1974},
  volume={9},
  number={5},
  pages={256-268},
}

@inproceedings{tvm-osdi18,
  title={{TVM}: An automated end-to-end optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={OSDI},
  year={2018}
}

@inproceedings{gandiva-osdi18,
	title        = {Gandiva: Introspective cluster scheduling for deep learning},
	author       = {Xiao, Wencong and Bhardwaj, Romil and Ramjee, Ramachandran and Sivathanu, Muthian and Kwatra, Nipun and Han, Zhenhua and Patel, Pratyush and Peng, Xuan and Zhao, Hanyu and Zhang, Quanlu and others},
	year         = 2018,
	booktitle    = {OSDI}
}

@inproceedings{optimus-eurosys18,
  title={Optimus: an efficient dynamic resource scheduler for deep learning clusters},
  author={Peng, Yanghua and Bao, Yixin and Chen, Yangrui and Wu, Chuan and Guo, Chuanxiong},
  booktitle={EuroSys},
  year={2018}
}

@inproceedings{tiresias-nsdi19,
	title        = {Tiresias: A {GPU} cluster manager for distributed deep learning},
	author       = {Gu, Juncheng and Chowdhury, Mosharaf and Shin, Kang G and Zhu, Yibo and Jeon, Myeongjae and Qian, Junjie and Liu, Hongqiang and Guo, Chuanxiong},
	year         = 2019,
	booktitle    = {NSDI}
}

@inproceedings{monet-iclr20,
  title={Memory Optimization for Deep Networks},
  author={Aashaka Shah and Chao-Yuan Wu and Jayashree Mohan and Vijay Chidambaram and Philipp Kraehenbuehl},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=bnY0jm4l59}
}

@misc{nvml,
	title        = {{{NVIDIA Management Library (NVML)}}},
	howpublished = {\url{https://developer.nvidia.com/nvidia-management-library-nvml}}
}

@misc{nvidia-smi,
	title        = {{{NVIDIA System Management Interface}}},
	howpublished = {\url{https://developer.nvidia.com/nvidia-system-management-interface}}
}

@misc{amdsmi,
	title        = {{{AMD System Management Interface}}},
	howpublished = {\url{https://rocm.docs.amd.com/projects/amdsmi/en/latest/}}
}

@misc{apple-netzero,
  title = {Apple Environment},
  howpublished = {https://www.apple.com/environment},
}

@misc{google-netzero,
  title = {Google Sustainability},
  howpublished = {https://sustainability.google},
}

@misc{meta-netzero,
  title = {Meta Climate},
  howpublished = {https://sustainability.fb.com/climate},
}

@misc{microsoft-netzero,
  title = {Microsoft Sustainability},
  howpublished = {https://www.microsoft.com/en-us/sustainability},
}

@misc{unep-report-2023,
  title = {Emissions Gap Report 2023},
  howpublished = {https://www.unep.org/resources/emissions-gap-report-2023},
  author = {United Nations Environment Programme},
}

@article{deepspeech-arxiv14,
  title={Deep speech: Scaling up end-to-end speech recognition},
  author={Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and others},
  journal={arXiv preprint arXiv:1412.5567},
  year={2014}
}

@inproceedings{imagenet-cvpr09,
  title={{ImageNet}: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  year={2009}
}

@article{albert-iclr20,
  title={{ALBERT}: A lite {BERT} for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={ICLR},
  year={2020}
}

@article{dlmedical,
  title={Deep learning in medical image analysis},
  author={Shen, Dinggang and Wu, Guorong and Suk, Heung-Il},
  journal={Annual review of biomedical engineering},
  volume={19},
  pages={221--248},
  year={2017},
  publisher={Annual Reviews}
}

@inproceedings{mlaas-nsdi22,
	title={{MLaaS} in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous {GPU} Clusters},
	author={Weng, Qizhen and Xiao, Wencong and Yu, Yinghao and Wang, Wei and Wang, Cheng and He, Jian and Li, Yong and Zhang, Liping and Lin, Wei and Ding, Yu},
	booktitle={NSDI},
	year={2022}
}

@inproceedings{resnet-cvpr16,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={CVPR},
	year={2016}
}

@inproceedings{dnn-recommendation-fb-hpca20,
	title={The architectural implications of facebook's {DNN}-based personalized recommendation},
	author={Gupta, Udit and Wu, Carole-Jean and Wang, Xiaodong and Naumov, Maxim and Reagen, Brandon and Brooks, David and Cottel, Bradford and Hazelwood, Kim and Hempstead, Mark and Jia, Bill and others},
	booktitle={HPCA},
	year={2020},
}

@inproceedings{applied-ml-at-fb-hpca18,
	title={Applied machine learning at {Facebook}: A datacenter infrastructure perspective},
	author={Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and others},
	booktitle={HPCA},
	year={2018},
}

@misc{nvidia-a100,
	title= {{{NVIDIA A100}}},
	howpublished={\url{https://www.nvidia.com/en-us/data-center/a100/}}
}

@article{patterson2021carbon,
	title={Carbon emissions and large neural network training},
	author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	journal={arXiv preprint arXiv:2104.10350},
	year={2021}
}

@article{gpu-dvfs-survey,
	title={A survey and measurement study of {GPU} {DVFS}  on energy conservation},
	author={Mei, Xinxin and Wang, Qiang and Chu, Xiaowen},
	journal={Digital Communications and Networks},
	volume={3},
	number={2},
	pages={89--100},
	year={2017},
	publisher={Elsevier}
}


@article{autonomous-driving1,
	title={Safe, multi-agent, reinforcement learning for autonomous driving},
	author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
	journal={arXiv preprint arXiv:1610.03295},
	year={2016}
}

@misc{apple-siri,
	title= {{{Siri - Apple}}},
	howpublished={\url{https://www.apple.com/siri/}}
}


@inproceedings{integrated-gpu-power-model-isca10,
	title={An integrated {GPU} power and performance model},
	author={Hong, Sunpyo and Kim, Hyesoon},
	booktitle={ISCA},
	year={2010}
}

@inproceedings{gpu-power-model-analytical,
	title={A performance and energy consumption analytical model for {GPU}},
	author={Luo, Cheng and Suda, Reiji},
	booktitle={2011 IEEE ninth international conference on dependable, autonomic and secure computing},
	year={2011},
}

@article{liang2022metashift,
	title={MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution Shifts and Training Conflicts},
	author={Liang, Weixin and Zou, James},
	journal={arXiv preprint arXiv:2202.06523},
	year={2022}
}

@article{learning-context-drift,
	title={Learning under concept drift: A review},
	author={Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
	journal={IEEE Transactions on Knowledge and Data Engineering},
	volume={31},
	number={12},
	pages={2346--2363},
	year={2018},
	publisher={IEEE}
}

@inproceedings{treehouse,
    author    = {Thomas Anderson and Adam Belay and Mosharaf Chowdhury and Asaf Cidon and Irene Zhang},
    booktitle = {HotCarbon},
    title     = {Treehouse: A Case For Carbon-Aware Datacenter Software},
    year      = {2022},
}

@inproceedings{sustainableai-mlsys22,
 author = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Aga, Fiona and Huang, Jinshi and Bai, Charles and Gschwind, Michael and Gupta, Anurag and Ott, Myle and Melnikov, Anastasia and Candido, Salvatore and Brooks, David and Chauhan, Geeta and Lee, Benjamin and Lee, Hsien-Hsin and Akyildiz, Bugra and Balandat, Maximilian and Spisak, Joe and Jain, Ravi and Rabbat, Mike and Hazelwood, Kim},
 booktitle = {Proceedings of Machine Learning and Systems},
 title = {Sustainable {AI}: Environmental Implications, Challenges and Opportunities},
 year = {2022}
}

@inproceedings{measuring-carbon,
  author = {Dodge, Jesse and Prewitt, Taylor and Tachet des Combes, Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A. and DeCario, Nicole and Buchanan, Will},
  title = {Measuring the Carbon Intensity of {AI} in Cloud Instances},
  year = {2022},
  booktitle = {ACM Conference on Fairness, Accountability, and Transparency},
}

@article{experiment-impact-tracker,
	title={Towards the systematic reporting of the energy and carbon footprints of machine learning},
	author={Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
	journal={Journal of Machine Learning Research},
	volume={21},
	number={248},
	pages={1--43},
	year={2020}
}

@misc{carbontracker,
  title={Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models},
  author={Lasse F. Wolff Anthony and Benjamin Kanding and Raghavendra Selvan},
  howpublished={ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems},
  year={2020}
}

@article{dawnbench-tta,
	title={Analysis of dawnbench, a time-to-accuracy machine learning performance benchmark},
	author={Coleman, Cody and Kang, Daniel and Narayanan, Deepak and Nardi, Luigi and Zhao, Tian and Zhang, Jian and Bailis, Peter and Olukotun, Kunle and R{\'e}, Chris and Zaharia, Matei},
	journal={ACM SIGOPS Operating Systems Review},
	year={2019},
}

@inproceedings{benchmark-ai-accelerators,
	title={Benchmarking the performance and energy efficiency of {AI} accelerators for {AI} training},
	author={Wang, Yuxin and Wang, Qiang and Shi, Shaohuai and He, Xin and Tang, Zhenheng and Zhao, Kaiyong and Chu, Xiaowen},
	booktitle={20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)},
	year={2020},
}

@inproceedings{gpu-70percent,
	title={Towards power efficiency in deep learning on data center hardware},
	author={Hodak, Miro and Gorkovenko, Masha and Dholakia, Ajay},
	booktitle={IEEE International Conference on Big Data},
	year={2019},
}

@article{nonstationary-bandit,
	title        = {Stochastic multi-armed-bandit problem with non-stationary rewards},
	author       = {Besbes, Omar and Gur, Yonatan and Zeevi, Assaf},
	year         = 2014,
	journal      = {NeurIPS}
}

@article{pytorch,
	title        = {Pytorch: An imperative style, high-performance deep learning library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
	year         = 2019,
	journal      = {NeurIPS}
}

@article{ts-tutorial,
	title={A tutorial on thompson sampling},
	author={Russo, Daniel J and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng and others},
	journal={Foundations and Trends{\textregistered} in Machine Learning},
	volume={11},
	number={1},
	pages={1--96},
	year={2018},
	publisher={Now Publishers, Inc.}
}

@article{ts-concurrent,
  title={A modern Bayesian look at the multi-armed bandit},
  author={Scott, Steven L},
  journal={Applied Stochastic Models in Business and Industry},
  volume={26},
  number={6},
  pages={639--658},
  year={2010},
  publisher={Wiley Online Library}
}

@article{conjugate,
  title={A Compendium of Conjugate Priors},
  author={Fink, Daniel},
  year={1997}
}

@inproceedings{narya,
	title        = {Predictive and Adaptive Failure Mitigation to Avert Production Cloud {VM} Interruptions},
	author       = {Levy, Sebastien and Yao, Randolph and Wu, Youjiang and Dang, Yingnong and Huang, Peng and Mu, Zheng and Zhao, Pu and Ramani, Tarun and Govindaraju, Naga and Li, Xukun and others},
	year         = 2020,
	booktitle    = {OSDI}
}

@inproceedings{marcus2021bao,
	title={Bao: Making learned query optimization practical},
	author={Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Tatbul, Nesime and Alizadeh, Mohammad and Kraska, Tim},
	booktitle={SIGMOD},
	year={2021}
}

@article{empirical-ts,
	title        = {An empirical evaluation of thompson sampling},
	author       = {Chapelle, Olivier and Li, Lihong},
	year         = 2011,
	journal      = {NeurIPS}
}

@article{ucb1985,
	title={Asymptotically efficient adaptive allocation rules},
	author={Lai, Tze Leung and Robbins, Herbert and others},
	journal={Advances in applied mathematics},
	volume={6},
	number={1},
	pages={4--22},
	year={1985}
}

@article{ucb2002,
	title={Finite-time analysis of the multiarmed bandit problem},
	author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
	journal={Machine learning},
	volume={47},
	number={2},
	pages={235--256},
	year={2002},
	publisher={Springer}
}

@article{thompson1933likelihood,
	title={On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
	author={Thompson, William R},
	journal={Biometrika},
	volume={25},
	number={3-4},
	pages={285--294},
	year={1933},
	publisher={Oxford University Press}
}

@inproceedings{ncf,
	title={Neural collaborative filtering},
	author={He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
	booktitle={Proceedings of the 26th international conference on world wide web},
	year={2017}
}

@article{megatronlm-arxiv,
  title={{Megatron-LM}: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{deepspeed-billionparam,
	title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
	author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
	booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages={3505--3506},
	year={2020}
}

@inproceedings{colossalai-icpp23,
author = {Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},
title = {Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},
year = {2023},
isbn = {9798400708435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605573.3605613},
doi = {10.1145/3605573.3605613},
abstract = {The success of Transformer models has pushed the deep learning model scale to billions of parameters, but the memory limitation of a single GPU has led to an urgent need for training on multi-GPU clusters. However, the best practice for choosing the optimal parallel strategy is still lacking, as it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism and is integrated with heterogeneous training and zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing},
pages = {766â€“775},
numpages = {10},
keywords = {text tagging, neural networks, gaze detection, datasets},
location = {<conf-loc>, <city>Salt Lake City</city>, <state>UT</state>, <country>USA</country>, </conf-loc>},
series = {ICPP '23}
}

@article{rnn-hardware-survey,
	title={A survey on hardware accelerators and optimization techniques for {RNNs}},
	author={Mittal, Sparsh and Umesh, Sumanth},
	journal={Journal of Systems Architecture},
	volume={112},
	pages={101839},
	year={2021},
	publisher={Elsevier}
}

@article{dnn-hardware-survey,
	title={Hardware and software optimizations for accelerating deep neural networks: Survey of current trends, challenges, and the road ahead},
	author={Capra, Maurizio and Bussolino, Beatrice and Marchisio, Alberto and Masera, Guido and Martina, Maurizio and Shafique, Muhammad},
	journal={IEEE Access},
	volume={8},
	pages={225134--225180},
	year={2020},
	publisher={IEEE}
}

@article{lr-scaling-prescription,
  author  = {Diego Granziol and Stefan Zohren and Stephen Roberts},
  title   = {Learning Rates as a Function of Batch Size: A Random Matrix Theory Approach to Neural Network Training},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {173},
  pages   = {1--65},
}

@inproceedings{lr-scaling-sqr,
	title        = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	author       = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	year         = 2017,
	booktitle    = {NeurIPS}
}

@article{lr-scaling-linear,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{electricity-dataset,
  title={Splice-2 comparative evaluation: Electricity pricing},
  author={Harries, Michael and Wales, New South},
  year={1999},
  publisher={Citeseer}
}

@inproceedings{airlines-dataset,
  title={Moa: Massive online analysis, a framework for stream classification and clustering},
  author={Bifet, Albert and Holmes, Geoff and Pfahringer, Bernhard and Kranen, Philipp and Kremer, Hardy and Jansen, Timm and Seidl, Thomas},
  booktitle={Proceedings of the first workshop on applications of pattern analysis},
  year={2010},
}

@inproceedings{circle-dataset,
  title={Learning with drift detection},
  author={Gama, Joao and Medas, Pedro and Castillo, Gladys and Rodrigues, Pedro},
  booktitle={Brazilian symposium on artificial intelligence},
  year={2004},
}

@inproceedings{hyperplane-dataset,
  title={Mining time-changing data streams},
  author={Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
  booktitle={Proceedings of the seventh ACM international conference on Knowledge discovery and data mining (SIGKDD)},
  year={2001}
}

@inproceedings{matchmaker,
	author = {Mallick, Ankur and Hsieh, Kevin and Arzani, Behnaz and Joshi, Gauri},
	booktitle = {MLSys},
	title = {Matchmaker: Data Drift Mitigation in Machine Learning for Large-Scale Systems},
	year = {2022}}

@article{adadelta,
	title={Adadelta: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}


@misc{cifar-url,
	title        = {{{CIFAR-10 and CIFAR-100 datasets}}},
	howpublished = {\url{https://www.cs.toronto.edu/~kriz/cifar.html}}
}

@article{cifar-paper,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	publisher={Citeseer}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (NAACL)",
    year = "2019",
}


@inproceedings{gpt3,
 title = {Language Models are Few-Shot Learners},
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {NeurIPS},
 year = {2020}
}

@inproceedings{shufflenetv2,
	title        = {Shufflenet v2: Practical guidelines for efficient {CNN} architecture design},
	author       = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
	year         = 2018,
	booktitle    = {ECCV}
}

@inproceedings{adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR},
}

@inproceedings{adamw,
	title={Decoupled weight decay regularization},
	author={Loshchilov, Ilya and Hutter, Frank},
	booktitle={ICLR},
	year={2019}
}

@article{movielens,
	title={The movielens datasets: History and context},
	author={Harper, F Maxwell and Konstan, Joseph A},
	journal={ACM transactions on interactive intelligent systems (TIIS)},
	volume={5},
	number={4},
	pages={1--19},
	year={2015},
	publisher={ACM New York, NY, USA}
}

@inproceedings{squad-paper,
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  title={{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
  year={2016},
  booktitle={EMNLP},
}

@inproceedings{librispeech,
	title={Librispeech: an {ASR} corpus based on public domain audio books},
	author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
	booktitle={IEEE international conference on acoustics, speech and signal processing (ICASSP)},
	year={2015},
}

@article{twitter-sentiment,
	title={Twitter sentiment classification using distant supervision},
	author={Go, Alec and Bhayani, Richa and Huang, Lei},
	journal={Stanford CS224N project report},
	year={2009}
}

@article{censor1977pareto,
	title={Pareto optimality in multiobjective problems},
	author={Censor, Yair},
	journal={Applied Mathematics and Optimization},
	volume={4},
	number={1},
	pages={41--59},
	year={1977},
	publisher={Springer}
}

@inproceedings{chameleoncloud,
	title={Lessons learned from the chameleon testbed},
	author={Keahey, Kate and Anderson, Jason and Zhen, Zhuo and Riteau, Pierre and Ruth, Paul and Stanzione, Dan and Cevik, Mert and Colleran, Jacob and Gunawi, Haryadi S and Hammock, Cody and others},
	booktitle={ATC},
	year={2020}
}

@inproceedings{cloudlab,
	title={The Design and Operation of {CloudLab}},
	author={Duplyakin, Dmitry and Ricci, Robert and Maricq, Aleksander and Wong, Gary and Duerig, Jonathon and Eide, Eric and Stoller, Leigh and Hibler, Mike and Johnson, David and Webb, Kirk and others},
	booktitle={ATC},
	year={2019}
}

@inproceedings{odpp-ccgrid20,
  title={Indicator-directed dynamic power management for iterative workloads on {GPU}-accelerated systems},
  author={Zou, Pengfei and Li, Ang and Barker, Kevin and Ge, Rong},
  booktitle={CCGRID},
  year={2020},
}

@article{gpoeo-tpds21,
  title={Dynamic {GPU} Energy Optimization for Machine Learning Training Workloads},
  author={Wang, Farui and Zhang, Weizhe and Lai, Shichao and Hao, Meng and Wang, Zheng},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  year={2021},
}

@inproceedings{dvfs-impact,
  title={The impact of {GPU} {DVFS} on the energy and performance of deep learning: An empirical study},
  author={Tang, Zhenheng and Wang, Yuxin and Wang, Qiang and Chu, Xiaowen},
  booktitle={Proceedings of the Tenth ACM International Conference on Future Energy Systems},
  year={2019}
}

@inproceedings{alert,
  title={{ALERT}: Accurate learning for energy and timeliness},
  author={Wan, Chengcheng and Santriaji, Muhammad and Rogers, Eri and Hoffmann, Henry and Maire, Michael and Lu, Shan},
  booktitle={ATC},
  year={2020}
}

@inproceedings{mobile-inference-energy-acc,
  title={Design considerations for energy-efficient inference on edge devices},
  author={Hanafy, Walid A and Molom-Ochir, Tergel and Shenoy, Rohan},
  booktitle={Proceedings of the Twelfth ACM International Conference on Future Energy Systems},
  pages={302--308},
  year={2021}
}


@inproceedings{edgebert,
	title        = {{EdgeBERT}: Sentence-level energy optimizations for latency-aware multi-task {NLP} inference},
	author       = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M and Brooks, David and others},
	year         = 2021,
	booktitle    = {MICRO}
}

@article{randomness-DNN,
  title={Randomness in neural networks: an overview},
  author={Scardapane, Simone and Wang, Dianhui},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={7},
  number={2},
  pages={e1200},
  year={2017},
  publisher={Wiley Online Library}
}

@inproceedings{ts-regret-bound,
  title={Further optimal regret bounds for thompson sampling},
  author={Agrawal, Shipra and Goyal, Navin},
  booktitle={Artificial intelligence and statistics},
  pages={99--107},
  year={2013},
  organization={PMLR}
}

@inproceedings{agrawal2012analysis,
  title={Analysis of thompson sampling for the multi-armed bandit problem},
  author={Agrawal, Shipra and Goyal, Navin},
  booktitle={Conference on learning theory},
  pages={39--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{largebatch1,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle={ICLR},
  year={2017}
}

@article{largebatch2,
  title={On the computational inefficiency of large batch sizes for stochastic gradient descent},
  author={Golmant, Noah and Vemuri, Nikita and Yao, Zhewei and Feinberg, Vladimir and Gholami, Amir and Rothauge, Kai and Mahoney, Michael W and Gonzalez, Joseph},
  journal={arXiv preprint arXiv:1811.12941},
  year={2018}
}

@article{smallbatch,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@inproceedings{huggingface,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "EMNLP",
    year = "2020",
}

@inproceedings{network-calculus,
  title={A calculus for network delay, Part I: Network elements in isolation},
  author={Rene L. Cruz},
  booktitle={IEEE/ACM Transactions on Information Theory},
  year={1991},
}

@article{gpu-nest,
  title={{GPU}-nest: Characterizing energy efficiency of multi-{GPU} inference servers},
  author={Jahanshahi, Ali and Sabzi, Hadi Zamani and Lau, Chester and Wong, Daniel},
  journal={IEEE Computer Architecture Letters},
  volume={19},
  number={2},
  pages={139--142},
  year={2020},
  publisher={IEEE}
}

@article{energy-clusterman,
  title={Cost Efficient {GPU} Cluster Management for Training and Inference of Deep Learning},
  author={Kang, Dong-Ki and Lee, Ki-Beom and Kim, Young-Chon},
  journal={Energies},
  volume={15},
  number={2},
  pages={474},
  year={2022},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{case-for-energy-proportional,
  title={The case for energy-proportional computing},
  author={Barroso, Luiz Andr{\'e} and H{\"o}lzle, Urs},
  journal={Computer},
  volume={40},
  number={12},
  pages={33--37},
  year={2007},
  publisher={IEEE}
}

@inproceedings{gradmatch-icml,
	title        = {Grad-match: Gradient matching based data subset selection for efficient deep model training},
	author       = {Killamsetty, Krishnateja and Durga, S and Ramakrishnan, Ganesh and De, Abir and Iyer, Rishabh},
	year         = 2021,
	booktitle    = {ICML}
}

@article{energy-nlp-policy,
  title={Energy and policy considerations for deep learning in {NLP}},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@article{quantify-carbon,
  title={Quantifying the carbon emissions of machine learning},
  author={Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  journal={arXiv preprint arXiv:1910.09700},
  year={2019}
}

@article{google-tpu,
  title={A domain-specific architecture for deep neural networks},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David},
  journal={Communications of the ACM},
  volume={61},
  number={9},
  pages={50--59},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@misc{us-household,
	title        = {{{How much electricity does an American home use?}}},
	howpublished = {\url{https://www.eia.gov/tools/faqs/faq.php?id=97&t=3}}
}

@inproceedings{build-ontopof-impact-tracker,
    title = "Towards Accurate and Reliable Energy Measurement of {NLP} Models",
    author = "Cao, Qingqing  and
      Balasubramanian, Aruna  and
      Balasubramanian, Niranjan",
    booktitle = "Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing",
    year = "2020",
}

@inproceedings {ansor,
author = {Lianmin Zheng and Chengfan Jia and Minmin Sun and Zhao Wu and Cody Hao Yu and Ameer Haj-Ali and Yida Wang and Jun Yang and Danyang Zhuo and Koushik Sen and Joseph E. Gonzalez and Ion Stoica},
title = {Ansor: Generating High-Performance Tensor Programs for Deep Learning},
booktitle = {OSDI},
year = {2020}
}

@inproceedings{taso,
	title        = {{TASO}: Optimizing Deep Learning Computation with Automatic Generation of Graph Substitutions},
	author       = {Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
	year         = 2019,
	booktitle    = {SOSP},
}
@inproceedings{PET,
	title        = {{PET}: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections},
	author       = {Haojie Wang and Jidong Zhai and Mingyu Gao and Zixuan Ma and Shizhi Tang and Liyan Zheng and Yuanzhi Li and Kaiyuan Rong and Yuanyong Chen and Zhihao Jia},
	year         = 2021,
	booktitle    = {OSDI}
}

@article{gpu-power-survey,
  title={Understanding GPU power: A survey of profiling, modeling, and simulation methods},
  author={Bridges, Robert A and Imam, Neena and Mintz, Tiffany M},
  journal={ACM Computing Surveys (CSUR)},
  volume={49},
  number={3},
  pages={1--27},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{estimation-energy-ml,
  title={Estimation of energy consumption in machine learning},
  author={Garc{\'\i}a-Mart{\'\i}n, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, H{\aa}kan},
  journal={Journal of Parallel and Distributed Computing},
  volume={134},
  pages={75--88},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{dvfs-taskmap-powercap,
  title={Power capping of {CPU}-{GPU} heterogeneous systems through coordinating {DVFS} and task mapping},
  author={Komoda, Toshiya and Hayashi, Shingo and Nakada, Takashi and Miwa, Shinobu and Nakamura, Hiroshi},
  booktitle={2013 IEEE 31st International Conference on computer design (ICCD)},
  year={2013},
  organization={IEEE}
}


@inproceedings{gpu-energy-adaptive-data-compression,
  title={Exploiting adaptive data compression to improve performance and energy-efficiency of compute workloads in multi-GPU systems},
  author={Tavana, Mohammad Khavari and Sun, Yifan and Agostini, Nicolas Bohm and Kaeli, David},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={664--674},
  year={2019},
  organization={IEEE}
}

@inproceedings{dynamic-underclock-gpu,
  title={DUB: Dynamic underclocking and bypassing in {NoCs} for heterogeneous {GPU} workloads},
  author={Bharadwaj, Srikant and Das, Shomit and Eckert, Yasuko and Oskin, Mark and Krishna, Tushar},
  booktitle={2021 15th IEEE/ACM International Symposium on Networks-on-Chip (NOCS)},
  year={2021},
}

@inproceedings{batchsizer,
  title={BatchSizer: Power-performance tradeoff for {DNN} inference},
  author={Nabavinejad, Seyed Morteza and Reda, Sherief and Ebrahimi, Masoumeh},
  booktitle={Proceedings of the 26th Asia and South Pacific Design Automation Conference},
  year={2021}
}

@inproceedings{gpgpu-power-estimate,
  title={GPGPU performance and power estimation using machine learning},
  author={Wu, Gene and Greathouse, Joseph L and Lyashevsky, Alexander and Jayasena, Nuwan and Chiou, Derek},
  booktitle={2015 IEEE 21st international symposium on high performance computer architecture (HPCA)},
  pages={564--576},
  year={2015},
  organization={IEEE}
}

@article{gpgpu-model-dvfs,
  title={GPGPU performance estimation with core and memory frequency scaling},
  author={Wang, Qiang and Chu, Xiaowen},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={31},
  number={12},
  pages={2865--2881},
  year={2020},
  publisher={IEEE}
}

@article{warmstart-training,
	title        = {On warm-starting neural network training},
	author       = {Ash, Jordan and Adams, Ryan P},
	year         = 2020,
	journal      = {NeurIPS}
}

@inproceedings{verified-gpu-instruction-energy-model,
  title={Verified instruction-level energy consumption measurement for {NVIDIA} {GPUs}},
  author={Arafa, Yehia and ElWazir, Ammar and ElKanishy, Abdelrahman and Aly, Youssef and Elsayed, Ayatelrahman and Badawy, Abdel-Hameed and Chennupati, Gopinath and Eidenbenz, Stephan and Santhi, Nandakishore},
  booktitle={Proceedings of the 17th ACM International Conference on Computing Frontiers},
  year={2020}
}

@inproceedings{accelwattch,
  title={{AccelWattch}: A Power Modeling Framework for Modern {GPUs}},
  author={Kandiah, Vijay and Peverelle, Scott and Khairy, Mahmoud and Pan, Junrui and Manjunath, Amogh and Rogers, Timothy G and Aamodt, Tor M and Hardavellas, Nikos},
  booktitle={MICRO},
  year={2021}
}

@inproceedings{pollux,
	title        = {Pollux: Co-adaptive cluster scheduling for goodput-optimized deep learning},
	author       = {Qiao, Aurick and Choe, Sang Keun and Subramanya, Suhas Jayaram and Neiswanger, Willie and Ho, Qirong and Zhang, Hao and Ganger, Gregory R and Xing, Eric P},
	year         = 2021,
	booktitle    = {OSDI}
}

@inproceedings{pipedream-sosp19,
	title        = {{PipeDream}: generalized pipeline parallelism for {DNN} training},
	author       = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
	year         = 2019,
	booktitle    = {SOSP}
}

@inproceedings{zero,
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={International Conference for High Performance Computing, Networking, Storage and Analysis (SC)}, 
  title={{ZeRO}: Memory optimizations Toward Training Trillion Parameter Models}, 
  year={2020},
}

@article{gspmd,
  title={{GSPMD}: general and scalable parallelization for {ML} computation graphs},
  author={Xu, Yuanzhong and Lee, HyoukJoong and Chen, Dehao and Hechtman, Blake and Huang, Yanping and Joshi, Rahul and Krikun, Maxim and Lepikhin, Dmitry and Ly, Andy and Maggioni, Marcello and others},
  journal={arXiv preprint arXiv:2105.04663},
  year={2021}
}

@inproceedings{oort,
	title        = {Oort: Efficient federated learning via guided participant selection},
	author       = {Lai, Fan and Zhu, Xiangfeng and Madhyastha, Harsha V and Chowdhury, Mosharaf},
	year         = 2021,
	booktitle    = {OSDI}
}

@inproceedings{bytescheduler,
	title        = {A generic communication scheduler for distributed {DNN} training acceleration},
	author       = {Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong},
	year         = 2019,
	booktitle    = {SOSP}
}

@inproceedings{blinkml,
	author = {Wang, Guanhua and Venkataraman, Shivaram and Phanishayee, Amar and Devanur, Nikhil and Thelin, Jorgen and Stoica, Ion},
	booktitle = {Proceedings of Machine Learning and Systems},
	title = {Blink: Fast and Generic Collectives for Distributed {ML}},
	year = {2020}
}

@article{gns,
  title={An empirical model of large-batch training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}

@article{kmeans,
  title={Algorithm AS 136: A k-means clustering algorithm},
  author={Hartigan, John A and Wong, Manchek A},
  journal={Journal of the royal statistical society. series c (applied statistics)},
  volume={28},
  number={1},
  pages={100--108},
  year={1979},
  publisher={JSTOR}
}

@article{imagenet1hour,
  title={Accurate, large minibatch {SGD}: Training {ImageNet} in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@inproceedings{incbs,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  booktitle={ICLR},
  year={2018}
}

@book{goodfellowdl,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{fluid,
  title={Fluid: Resource-aware Hyperparameter Tuning Engine},
  author={Yu, Peifeng and Liu, Jiachen and Chowdhury, Mosharaf},
  journal={MLSys},
  year={2021}
}

@article{hpo,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={NeurIPS},
  year={2011}
}

@article{asha,
  title={A system for massively parallel hyperparameter tuning},
  author={Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Ben-Tzur, Jonathan and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={230--246},
  year={2020}
}

@article{hyperband,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6765--6816},
  year={2017},
  publisher={JMLR. org}
}

@article{minlp,
  title={Mixed-integer nonlinear optimization},
  volume={22},
  DOI={10.1017/S0962492913000032},
  journal={Acta Numerica},
  publisher={Cambridge University Press},
  author={Belotti, Pietro and Kirches, Christian and Leyffer, Sven and Linderoth, Jeff and Luedtke, James and Mahajan, Ashutosh},
  year={2013},
  pages={1â€“131}
}

@inproceedings{pipedream-2bw,
	title        = {Memory-Efficient Pipeline-Parallel {DNN} Training},
	author       = {Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
	year         = 2021,
	booktitle    = {ICML}
}

@inproceedings{megatronlm-sc21,
	title        = {Efficient Large-Scale Language Model Training on {GPU} Clusters Using {Megatron-LM}},
	author       = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
	year         = 2021,
	booktitle    = {SC}
}

@inproceedings{zeus-nsdi23,
    author    = {Jie You and Jae-Won Chung and Mosharaf Chowdhury},
    booktitle = {USENIX NSDI},
    title     = {Zeus: Understanding and Optimizing {GPU} Energy Consumption of {DNN} Training},
    year      = {2023},
}

@InProceedings{autopipe-icml21,
  title = 	 {{PipeTransformer}: Automated Elastic Pipelining for Distributed Training of Large-Scale Models},
  author =       {He, Chaoyang and Li, Shen and Soltanolkotabi, Mahdi and Avestimehr, Salman},
  booktitle = 	 {ICML},
  year = 	 {2021},
}

@inproceedings{gpipe-neurips19,
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  title = {{GPipe}: Efficient Training of Giant Neural Networks Using Pipeline Parallelism},
  year = {2019},
  booktitle = {NeurIPS},
}

@inproceedings{transformer-neurips17,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is All You Need},
year = {2017},
booktitle = {NeurIPS},
}

@inproceedings{alpa-osdi22,
author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
booktitle = {USENIX OSDI},
year = {2022},
}

@article{t5-jmlr20,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@InProceedings{pipedreamflush-icml21,
  title     = {Memory-Efficient Pipeline-Parallel {DNN} Training},
  author    = {Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
  booktitle = {ICML},
  year      = {2021},
}

@article{merak-tpds23,
  author={Lai, Zhiquan and Li, Shengwei and Tang, Xudong and Ge, Keshi and Liu, Weijie and Duan, Yabo and Qiao, Linbo and Li, Dongsheng},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={Merak: An Efficient Distributed {DNN} Training Framework With Automated 3D Parallelism for Giant Foundation Models}, 
  year={2023},
  volume={34},
  number={5},
  pages={1466-1478},
}

@misc{softwarebloat-wikipedia,
   author={Wikipedia},
   title={Software bloat},
   howpublished={\url{http://en.wikipedia.org/w/index.php?title=Software\%20bloat&oldid=1138976904}},
   note={[Online; accessed 15-March-2023]}
 }

@inproceedings{ugc-stoc02,
  author = {Khot, Subhash},
  title = {On the power of unique 2-prover 1-round games},
  year = {2002},
  booktitle = {STOC},
}

@inproceedings{dapple-ppopp21,
  author = {Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and Diao, Lansong and Liu, Xiaoyong and Lin, Wei},
  title = {{DAPPLE}: A Pipelined Data Parallel Approach for Training Large Models},
  year = {2021},
  booktitle = {ACM PPoPP},
}

@inproceedings{swin-iccv21,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  year={2021}
}

@misc{swin-github,
	title        = {{Swin-Transformer}},
	howpublished = {\url{https://github.com/microsoft/Swin-Transformer}}
}

@misc{deepspeed-github,
	title        = {{DeepSpeed}},
	howpublished = {\url{https://github.com/microsoft/DeepSpeed}}
}

@misc{megatronlm-github,
	title        = {{Megatron-LM}},
	howpublished = {\url{https://github.com/NVIDIA/Megatron-LM}}
}

@misc{pippy-github,
	title        = {{PyTorch PiPPy}},
	howpublished = {\url{https://github.com/pytorch/tau}}
}

@misc{fairseq-github,
	title        = {{FairSeq}},
	howpublished = {\url{https://github.com/facebookresearch/fairseq}}
}

@misc{fastapi-github,
  title        = {{FastAPI}},
  howpublished = {\url{https://github.com/tiangolo/fastapi}}
}

@book{nonlinear-multiobj-opt,
  Title                    = {Nonlinear Multiobjective Optimization},
  Author                   = {Miettinen Kaisa},
  Publisher                = {Kluwer Academic Publishers},
  Year                     = {1999},
  Address                  = {Boston, USA},
  Series                   = {International Series in Operations Research \& Management Science},
  Volume                   = {12}
}

@article{pd-mnsc77,
  author = {Phillips, Steve and Dessouky, Mohamed I.},
  title = {Solving the Project Time/Cost Tradeoff Problem Using the Minimal Cut Concept},
  journal = {Management Science},
  volume = {24},
  number = {4},
  pages = {393-400},
  year = {1977},
}

@article{pd-caie16,
  title = {A polynomial time repeated cuts algorithm for the time cost tradeoff problem: The linear and convex crashing cost deadline problem},
  journal = {Computers \& Industrial Engineering},
  volume = {95},
  pages = {64-71},
  year = {2016},
  author = {Dorit S. Hochbaum},
}

@phdthesis{skutella-thesis,
  title={Approximation and randomization in scheduling},
  author={Skutella, Martin},
  year={1998},
}

@article{skutella1998,
 author = {Martin Skutella},
 journal = {Mathematics of Operations Research},
 number = {4},
 pages = {909--929},
 title = {Approximation Algorithms for the Discrete Time-Cost Tradeoff Problem},
 volume = {23},
 year = {1998}
}

@inproceedings{gandivafair-eurosys20,
  author = {Chaudhary, Shubham and Ramjee, Ramachandran and Sivathanu, Muthian and Kwatra, Nipun and Viswanatha, Srinidhi},
  title = {Balancing Efficiency and Fairness in Heterogeneous {GPU} Clusters for Deep Learning},
  year = {2020},
  booktitle = {EuroSys},
}

@inproceedings {whale-atc22,
  author = {Xianyan Jia and Le Jiang and Ang Wang and Wencong Xiao and Ziji Shi and Jie Zhang and Xinyuan Li and Langshi Chen and Yong Li and Zhen Zheng and Xiaoyong Liu and Wei Lin},
  title = {Whale: Efficient Giant Model Training over Heterogeneous {GPUs}},
  booktitle = {ATC},
  year = {2022},
}

@phdthesis{phillips1974,
  title = {A Cut Search Approach To Max-Flow Min-Cut Problems And Cost Duration Analysis},
  author = {Phillips, Steve},
  year = {1974},
}

@article{projectsched1999,
  title = {Resource-constrained project scheduling: Notation, classification, models, and methods},
  journal = {European Journal of Operational Research},
  volume = {112},
  number = {1},
  pages = {3-41},
  year = {1999},
  author = {Peter Brucker and Andreas Drexl and Rolf MÃ¶hring and Klaus Neumann and Erwin Pesch},
}

@article{edmondskarp1972,
  author = {Edmonds, Jack and Karp, Richard M.},
  title = {Theoretical Improvements in Algorithmic Efficiency for Network Flow Problems},
  year = {1972},
  volume = {19},
  number = {2},
  journal = {Journal of the ACM},
  pages = {248â€“264},
}

@article{scalinglaws-arxiv20,
  title={Scaling Laws for Neural Language Models},
  author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{chinchilla-neurips22,
 author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Thomas and Noland, Eric and Millican, Katherine and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Kar\'{e}n and Elsen, Erich and Vinyals, Oriol and Rae, Jack and Sifre, Laurent},
 booktitle = {NeurIPS},
 title = {An empirical analysis of compute-optimal large language model training},
 year = {2022}
}

@inproceedings{wideresnet-bmvc16,
  title={Wide Residual Networks},
  author={Sergey Zagoruyko and Nikos Komodakis},
  year={2016},
  booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
}

@inproceedings{vit-iclr21,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle={ICLR},
  year={2021},
}

@misc{boundedflow,
  title={Extensions of Maximum Flow},
  author={Jeff Erickson},
  howpublished={\url{https://courses.engr.illinois.edu/cs498dl1/sp2015/notes/25-maxflowext.pdf}},
  note={[Online; accessed 05-April-2023]}
}

@book{fordfulkerson,
 author = {L. R. Ford and D. R. Fulkerson},
 publisher = {Princeton University Press},
 title = {Flows in Networks},
 year = {1962}
}

@software{jax,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  year = {2018},
}

@inproceedings{dodge-faact22,
  author = {Dodge, Jesse and Prewitt, Taylor and Tachet des Combes, Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A. and DeCario, Nicole and Buchanan, Will},
  title = {Measuring the Carbon Intensity of AI in Cloud Instances},
  year = {2022},
  booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
}

@article{bloomcarbon-arxiv23,
  title={Estimating the Carbon Footprint of {BLOOM}, a 176B Parameter Language Model}, 
  author={Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
  year={2022},
  eprint={arXiv preprint arXiv:2211.02001},
}

@article{checkpointing-arxiv16,
  title={Training Deep Nets with Sublinear Memory Cost}, 
  author={Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin},
  year={2016},
  eprint={arXiv preprint arXiv:1604.06174},
}

@article{chase-ccai23,
  title={Chasing Low-Carbon Electricity for Practical and Sustainable DNN Training}, 
  author={Zhenning Yang and Luoxi Meng and Jae-Won Chung and Mosharaf Chowdhury},
  year={2023},
  eprint={arXiv preprint arXiv:2303.02508},
}

@article{depo-future23,
  title = {Dynamic {GPU} power capping with online performance tracing for energy efficient {GPU} computing using {DEPO} tool},
  journal = {Future Generation Computer Systems},
  volume = {145},
  pages = {396-414},
  year = {2023},
  author = {Adam Krzywaniak and PaweÅ‚ Czarnul and Jerzy Proficz},
}

@article{de1997,
  author = {De, Prabuddha and Dunne, E. James and Ghosh, Jay B. and Wells, Charles E.},
  title = {Complexity of the Discrete Time-Cost Tradeoff Problem for Project Networks},
  journal = {Operations Research},
  volume = {45},
  number = {2},
  pages = {302-306},
  year = {1997},
}

@article{daboul2023,
  author = {Siad Daboul and Stephan Held and Jens Vygen},
  title = {Approximating the discrete time-cost tradeoff problem with bounded depth},
  journale = {Mathematical Programming},
  volume = {197},
  pages = {529â€“547},
  year = {2022},
}

@inproceedings{orlin2013,
  author = {Orlin, James B.},
  title = {Max Flows in {O(nm)} Time, or Better},
  year = {2013},
  booktitle = {ACM STOC}
}

@article{alpaserve-arxiv23,
  title={{AlpaServe}: Statistical Multiplexing with Model Parallelism for Deep Learning Serving}, 
  author={Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
  year={2023},
  eprint={arXiv preprint arXiv:2302.11665},
}

@article{megatron-turing-arxiv22,
  title={Using DeepSpeed and Megatron to Train {Megatron-Turing} {NLG} 530B, A {Large-Scale} Generative Language Model}, 
  author={Shaden Smith and Mostofa Patwary and Brandon Norick and Patrick LeGresley and Samyam Rajbhandari and Jared Casper and Zhun Liu and Shrimai Prabhumoye and George Zerveas and Vijay Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and Julie Bernauer and Xia Song and Mohammad Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro},
  year={2022},
  eprint={arXiv preprint arXiv:2201.11990},
}

@inproceedings {graphene-osdi16,
  author = {Robert Grandl and Srikanth Kandula and Sriram Rao and Aditya Akella and Janardhan Kulkarni},
  title = {{GRAPHENE}: Packing and {Dependency-Aware} Scheduling for {Data-Parallel} Clusters},
  booktitle = {USENIX OSDI},
  year = {2016},
}

@inproceedings {carbyne-osdi16,
  author = {Robert Grandl and Mosharaf Chowdhury and Aditya Akella and Ganesh Ananthanarayanan},
  title = {Altruistic Scheduling in {Multi-Resource} Clusters},
  booktitle = {USENIX OSDI},
  year = {2016},
}

@inproceedings{tetris-sigcomm14,
  author = {Grandl, Robert and Ananthanarayanan, Ganesh and Kandula, Srikanth and Rao, Sriram and Akella, Aditya},
  title = {{Multi-Resource} Packing for Cluster Schedulers},
  year = {2014},
  booktitle = {ACM SIGCOMM}
}

@inproceedings{monotasks-sosp17,
  author = {Ousterhout, Kay and Canel, Christopher and Ratnasamy, Sylvia and Shenker, Scott},
  title = {Monotasks: Architecting for Performance Clarity in Data Analytics Frameworks},
  year = {2017},
  booktitle = {SOSP},
}

@inproceedings{ursa-eurosys20,
  author = {Jin, Tatiana and Cai, Zhenkun and Li, Boyang and Zheng, Chengguang and Jiang, Guanxian and Cheng, James},
  title = {Improving Resource Utilization by Timely {Fine-Grained} Scheduling},
  year = {2020},
  booktitle = {EuroSys},
}

@inproceedings{stochasticparrot-facct21,
  author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  year = {2021},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21)},
}

@inproceedings{envpipe-atc23,
  author = {Sangjin Choi and Inhoe Koo and Jeongseob Ahn and Myeongjae Jeon and Youngjin Kwon},
  title = {{EnvPipe}: Performance-preserving {DNN} Training Framework for Saving Energy},
  year = {2023},
  booktitle = {ATC},
}

@inproceedings{thunderbolt-osdi20,
  author = {Shaohong Li and Xi Wang and Xiao Zhang and Vasileios Kontorinis and Sreekumar Kodakara and David Lo and Parthasarathy Ranganathan},
  title = {Thunderbolt: {Throughput-Optimized}, {Quality-of-Service-Aware} Power Capping at Scale},
  booktitle = {OSDI},
  year = {2020},
}

@inproceedings{mvpp-asplos20,
  author = {Sakalkar, Varun and Kontorinis, Vasileios and Landhuis, David and Li, Shaohong and De Ronde, Darren and Blooming, Thomas and Ramesh, Anand and Kennedy, James and Malone, Christopher and Clidaras, Jimmy and Ranganathan, Parthasarathy},
  title = {Data Center Power Oversubscription with a Medium Voltage Power Plane and {Priority-Aware} Capping},
  year = {2020},
  booktitle = {ASPLOS},
}

@article{polca-asplos24,
  author = {Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Goiri, \'{I}\~{n}igo and Warrier, Brijesh and Mahalingam, Nithish and Bianchini, Ricardo},
  title = {Characterizing Power Management Opportunities for LLMs in the Cloud},
  year = {2024},
  booktitle = {ASPLOS},
}

@article{bloom-arxiv22,
  title={{BLOOM}: A 176B-Parameter Open-Access Multilingual Language Model}, 
  author={BigScience Workshop},
  year={2023},
  eprint={arXiv preprint arXiv:2211.05100},
}

@inproceedings{vitscalinglaws-cvpr22,
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  title= {Scaling Vision Transformers},
  booktitle={CVPR},
  year={2022},
}

@article{llama2-arxiv23,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={GenAI, Meta},
  year={2023},
  eprint={arXiv preprint arXiv:2307.09288},
}

@misc{pytorch-stragglers,
  title = {Straggler Mitigation On PyTorch DDP By Hierarchical SGD},
  author = {Wang, Yi and Varma, Rohan},
  howpublished = {https://pytorch.org/blog/straggler-mitigation},
}


@inproceedings{varuna-eurosys22,
  author = {Athlur, Sanjith and Saran, Nitika and Sivathanu, Muthian and Ramjee, Ramachandran and Kwatra, Nipun},
  title = {Varuna: Scalable, Low-Cost Training of Massive Deep Learning Models},
  year = {2022},
  booktitle = {EuroSys},
}

@inproceedings{bamboo-nsdi23,
  author = {John Thorpe and Pengzhan Zhao and Jonathan Eyolfson and Yifan Qiao and Zhihao Jia and Minjia Zhang and Ravi Netravali and Guoqing Harry Xu},
  title = {Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large {DNNs}},
  booktitle = {NSDI},
  year = {2023},
}

@inproceedings{oobleck-sosp23,
    author           = {Jang, Insu and Yang, Zhenning and Zhang, Zhen and Jin, Xin and Chowdhury, Mosharaf},
    booktitle        = {SOSP},
    title            = {Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates},
    year             = {2023},
}

@inproceedings{recsys-dsi-isca22,
  author = {Zhao, Mark and Agarwal, Niket and Basant, Aarti and Gedik, Bu\u{g}ra and Pan, Satadru and Ozdal, Mustafa and Komuravelli, Rakesh and Pan, Jerry and Bao, Tianshu and Lu, Haowei and Narayanan, Sundaram and Langman, Jack and Wilfong, Kevin and Rastogi, Harsha and Wu, Carole-Jean and Kozyrakis, Christos and Pol, Parik},
  title = {Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training: Industrial Product},
  year = {2022},
  booktitle = {ISCA},
}

@article{datastalls-vldb21,
  author = {Mohan, Jayashree and Phanishayee, Amar and Raniwala, Ashish and Chidambaram, Vijay},
  title = {Analyzing and Mitigating Data Stalls in DNN Training},
  year = {2021},
  publisher = {VLDB Endowment},
  volume = {14},
  number = {5},
  month = {jan},
  pages = {771â€“784},
}

@article{gpupower-cal23,
  author  = {Patel, Pratyush and Gong, Zibo and Rizvi, Syeda and Choukse, Esha and Misra, Pulkit and Anderson, Thomas and Sriraman, Akshitha},
  journal = {IEEE Computer Architecture Letters},
  title   = {Towards Improved Power Management in Cloud GPUs},
  year    = {2023},
  volume  = {22},
  number  = {2},
  pages   = {141-144},
  doi     = {10.1109/LCA.2023.3278652}
}


@article{A100powerthermal-energies21,
  author         = {Å peÅ¥ko, Matej and VysockÃ½, OndÅ™ej and JansÃ­k, Branislav and Å˜Ã­ha, LubomÃ­r},
  title          = {{DGX-A100} Face to Face {DGX}-2 -- Performance, Power and Thermal Behavior Evaluation},
  journal        = {Energies},
  volume         = {14},
  year           = {2021},
  number         = {2},
  article-number = {376},
  url            = {https://www.mdpi.com/1996-1073/14/2/376},
  issn           = {1996-1073},
}

@inproceedings{largescalefailures-sc17,
  author = {Gupta, Saurabh and Patel, Tirthak and Engelmann, Christian and Tiwari, Devesh},
  title = {Failures in Large Scale Systems: Long-Term Measurement, Analysis, and Implications},
  year = {2017},
  booktitle = {SC},
}

@inproceedings{philly-atc19,
  author = {Myeongjae Jeon and Shivaram Venkataraman and Amar Phanishayee and Junjie Qian and Wencong Xiao and Fan Yang},
  title = {Analysis of {Large-Scale} {Multi-Tenant} {GPU} Clusters for {DNN} Training Workloads},
  booktitle = {ATC},
  year = {2019},
}

@InProceedings{dtct-approx-hardness,
author="Svensson, Ola",
editor="Gupta, Anupam
and Jansen, Klaus
and Rolim, Jos{\'e}
and Servedio, Rocco",
title="Hardness of Vertex Deletion and Project Scheduling",
booktitle="Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="301--312",
isbn="978-3-642-32512-0"
}

@misc{memorywall,
  author = {Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Mahoney, Michael and Keutzer, Kurt},
  title = {{AI} and Memory Wall},
  howpublished = {https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8},
}

@misc{H100whitepaper,
  author = {NVIDIA},
  title = {NVIDIA {H100} Tensor Core {GPU} Architecture Overview},
  howpublished = {https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper},
}

@misc{blackwell-news,
  author = {Tobias Mann},
  title = {{NVIDIA} turns up the {AI} heat with 1,200W Blackwell GPUs},
  howpublished = {https://www.theregister.com/2024/03/18/nvidia\_turns\_up\_the\_ai},
}

@misc{mlperf_training_31,
  author = {ML COMMONS},
  title = {{MLPerf} Training v3.1 benchmark results},
  howpublished = {https://github.com/mlcommons/training\_results\_v3.1},
}

@inproceedings{efficientscaling_mlsys22,
 author = {Park, Seo Jin and Fried, Joshua and Kim, Sunghyun and Alizadeh, Mohammad and Belay, Adam},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {D. Marculescu and Y. Chi and C. Wu},
 pages = {748--761},
 title = {Efficient Strong Scaling Through Burst Parallel Training},
 url = {https://proceedings.mlsys.org/paper_files/paper/2022/file/b99e69074b2fa1d8c8fe0d5b60e19397-Paper.pdf},
 volume = {4},
 year = {2022}
}


@INPROCEEDINGS{zero_sc20,
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={ZeRO: Memory optimizations Toward Training Trillion Parameter Models}, 
  year={2020},
  volume={},
  number={},
  pages={1-16},
  doi={10.1109/SC41405.2020.00024}
}

@misc{pytorchfsdp_arxiv,
  title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, 
  author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
  year={2023},
  eprint={arXiv preprint arXiv:2304.11277},
}

@misc{pytorchlightning,
  author = {Lightning-AI},
  title = {Pytorch Lightning},
  howpublished = {https://lightning.ai/pytorch-lightning},
}

@misc{mosaicml,
  author = {MosaicML},
  title = {MosaicML Training},
  howpublished = {https://www.mosaicml.com/training}}

@misc{gspmd_arxiv2021,
  title={GSPMD: General and Scalable Parallelization for ML Computation Graphs}, 
  author={Yuanzhong Xu and HyoukJoong Lee and Dehao Chen and Blake Hechtman and Yanping Huang and Rahul Joshi and Maxim Krikun and Dmitry Lepikhin and Andy Ly and Marcello Maggioni and Ruoming Pang and Noam Shazeer and Shibo Wang and Tao Wang and Yonghui Wu and Zhifeng Chen},
  year={2021},
  eprint={arXiv preprint arXiv:2105.04663},
}

@inproceedings{spmd_socc22,
  author = {Zhang, Shiwei and Diao, Lansong and Wu, Chuan and Wang, Siyu and Lin, Wei},
  title = {Accelerating Large-Scale Distributed Neural Network Training with SPMD Parallelism},
  year = {2022},
  booktitle = {SoCC},
}

@inproceedings{mantri-osdi10,
  author = {Ganesh Ananthanarayanan and Srikanth Kandula and Albert Greenberg and Ion Stoica and Yi Lu and Bikas Saha and Edward Harris},
  title = {Reining in the Outliers in {Map-Reduce} Clusters using Mantri},
  booktitle = {OSDI},
  year = {2010},
}

@inproceedings{megascale-nsdi24,
  author = {Ziheng Jiang and Haibin Lin and Yinmin Zhong and Qi Huang and Yangrui Chen and Zhi Zhang and Yanghua Peng and Xiang Li and Cong Xie and Shibiao Nong and Yulu Jia and Sun He and Hongmin Chen and Zhihao Bai and Qi Hou and Shipeng Yan and Ding Zhou and Yiyao Sheng and Zhuo Jiang and Haohan Xu and Haoran Wei and Zhang Zhang and Pengfei Nie and Leqi Zou and Sida Zhao and Liang Xiang and Zherui Liu and Zhe Li and Xiaoying Jia and Jianxi Ye and Xin Jin and Xin Liu},
  title = {{MegaScale}: Scaling Large Language Model Training to More Than 10,000 {GPUs}},
  booktitle = {NSDI},
  year = {2024},
}

@misc{hamilton2024constraint,
  title = {Constraint-Driven Innovation ({CIDR} 2024 keynote talk)},
  author = {James Hamilton},
  howpublished = {\url{https://mvdirona.com/jrh/talksandpapers/JamesHamiltonCIDR2024.pdf}},
}

@misc{cbre2023,
  title = {Global Data Center Trends 2023},
  author = {{CBRE}},
  year = {2023},
  howpublished = {\url{https://www.cbre.com/insights/reports/global-data-center-trends-2023}},
}

@misc{cbre2024,
  title = {Global Data Center Trends 2024},
  author = {{CBRE}},
  year = {2024},
  howpublished = {\url{https://www.cbre.com/insights/reports/global-data-center-trends-2024}},
}

@misc{mckinsey2023,
  title = {Investing in the Rising Data Center Economy},
  author = {{McKinsey \& Company}},
  year = {2023},
  howpublished = {\url{https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy#/}},
}

@article{eyeriss-jssc16,
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}, 
  year={2017},
  volume={52},
  number={1},
  pages={127-138},
}

@article{vivienne17efficient,
  author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  journal={Proceedings of the IEEE}, 
  title={Efficient Processing of Deep Neural Networks: A Tutorial and Survey}, 
  year={2017},
  volume={105},
  number={12},
  pages={2295-2329},
}

@inproceedings{eden-micro19,
  author = {Koppula, Skanda and Orosa, Lois and Ya\u{g}l\i{}k\c{c}\i{}, A. Giray and Azizi, Roknoddin and Shahroodi, Taha and Kanellopoulos, Konstantinos and Mutlu, Onur},
  title = {{EDEN}: Enabling Energy-Efficient, High-Performance Deep Neural Network Inference Using Approximate {DRAM}},
  year = {2019},
  booktitle = {MICRO},
}

@inproceedings{puma-asplos19,
  author = {Ankit, Aayush and Hajj, Izzat El and Chalamalasetti, Sai Rahul and Ndu, Geoffrey and Foltin, Martin and Williams, R. Stanley and Faraboschi, Paolo and Hwu, Wen-mei W and Strachan, John Paul and Roy, Kaushik and Milojicic, Dejan S.},
  title = {{PUMA}: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference},
  year = {2019},
  booktitle = {ASPLOS},
}

@inproceedings{snafu-isca21,
  author={Gobieski, Graham and Atli, Ahmet Oguz and Mai, Kenneth and Lucia, Brandon and Beckmann, Nathan},
  booktitle={ISCA}, 
  title={{SNAFU}: An Ultra-Low-Power, Energy-Minimal {CGRA}-Generation Framework and Architecture}, 
  year={2021},
  pages={1027-1040},
}

@inproceedings{riptide-micro22,
  author={Gobieski, Graham and Ghosh, Souradip and Heule, Marijn and Mowry, Todd and Nowatzki, Tony and Beckmann, Nathan and Lucia, Brandon},
  booktitle={MICRO}, 
  title={{RipTide}: A Programmable, Energy-Minimal Dataflow Compiler and Architecture}, 
  year={2022},
}

@inproceedings{motionplanning-isca23,
  author = {Shah, Deval and Yang, Ningfeng and Aamodt, Tor M.},
  title = {Energy-Efficient Realtime Motion Planning},
  year = {2023},
  booktitle = {ISCA},
}

@article{kws-jssc24,
  author       = {Heejin Yang and Ji{-}Hwan Seol and Rohit Rothe and Zichen Fan and Qirui Zhang and Hun{-}Seok Kim and David T. Blaauw and Dennis Sylvester},
  title        = {A 1.5-{$\mu$W} Fully-Integrated Keyword Spotting SoC in 28-nm {CMOS} With Skip-RNN and Fast-Settling Analog Frontend for Adaptive Frame Skipping},
  journal      = {{IEEE} J. Solid State Circuits},
  volume       = {59},
  number       = {1},
  pages        = {29--39},
  year         = {2024},
}

@inproceedings{enemy-hpca23,
  author={Yu, Junyeol and Kim, Jongseok and Seo, Euiseong},
  booktitle={HPCA}, 
  title={Know Your Enemy To Save Cloud Energy: Energy-Performance Characterization of Machine Learning Serving}, 
  year={2023},
}

@article{falcon-arxiv23,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}

@article{gemma-arxiv24,
  title={Gemma: Open models based on {Gemini} research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@inproceedings{swarm-icml23,
  author = {Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
  title = {{SWARM} parallelism: training large models can be surprisingly communication-efficient},
  year = {2023},
  booktitle = {ICML},
}

@software{gpt-neox-github,
  title = {{GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch}},
  author = {Andonian, Alex and Anthony, Quentin and Biderman, Stella and Black, Sid and Gali, Preetham and Gao, Leo and Hallahan, Eric and Levy-Kramer, Josh and Leahy, Connor and Nestler, Lucas and Parker, Kip and Pieler, Michael and Phang, Jason and Purohit, Shivanshu and Schoelkopf, Hailey and Stander, Dashiell and Songz, Tri and Tigges, Curt and ThÃ©rien, Benjamin and Wang, Phil and Weinbach, Samuel},
  url = {https://www.github.com/eleutherai/gpt-neox},
  doi = {10.5281/zenodo.5879544},
  month = {9},
  year = {2023},
  version = {2.0.0},
}

@inproceedings{svensson2012hardness,
  title={Hardness of vertex deletion and project scheduling},
  author={Svensson, Ola},
  booktitle={International Workshop on Approximation Algorithms for Combinatorial Optimization},
  pages={301--312},
  year={2012},
  organization={Springer}
}

@inproceedings{tpuv4-nsdi24,
  author = {Yazhou Zu and Alireza Ghaffarkhah and Hoang-Vu Dang and Brian Towles and Steven Hand and Safeen Huda and Adekunle Bello and Alexander Kolbasov and Arash Rezaei and Dayou Du and Steve Lacy and Hang Wang and Aaron Wisner and Chris Lewis and Henri Bahini},
  title = {Resiliency at Scale: Managing {Google's} {TPUv4} Machine Learning Supercomputer},
  booktitle = {NSDI},
  year = {2024},
}

@misc{llama3-blog,
  title = {Introducing {Meta} {Llama} 3: The most capable openly available {LLM} to date},
  author = {{Meta AI}},
  howpublished = {\url{https://ai.meta.com/blog/meta-llama-3/}},
}

@inproceedings{clockwork-osdi20,
  author = {Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace},
  title = {Serving {DNNs} like Clockwork: Performance Predictability from the Bottom Up},
  booktitle = {OSDI},
  year = {2020},
}

@inproceedings{antdt-icde24,
  author = {Y. Xiao and L. Ju and Z. Zhou and S. Li and Z. Huan and D. Zhang and R. Jiang and L. Wang and X. Zhang and L. Liang and J. Zhou},
  booktitle = {ICDE},
  title = {AntDT: A Self-Adaptive Distributed Training Framework for Leader and Straggler Nodes},
  year = {2024},
}

@inproceedings{nnscaler-osdi24,
author = {Zhiqi Lin and Youshan Miao and Quanlu Zhang and Fan Yang and Yi Zhu and Cheng Li and Saeed Maleki and Xu Cao and Ning Shang and Yilei Yang and Weijiang Xu and Mao Yang and Lintao Zhang and Lidong Zhou},
title = {{nnScaler}: {Constraint-Guided} Parallelization Plan Generation for Deep Learning Training},
booktitle = {OSDI},
year = {2024},
}

@inproceedings{mapreduce-dvfs-igcc11,
  author={Wirtz, Thomas and Ge, Rong},
  booktitle={International Green Computing Conference and Workshops}, 
  title={Improving {MapReduce} energy efficiency for computation intensive workloads}, 
  year={2011},
}

@article{hadoop-dvfs-fgcs16,
  title = {Governing energy consumption in Hadoop through CPU frequency scaling: An analysis},
  journal = {Future Generation Computer Systems},
  volume = {54},
  pages = {219-232},
  year = {2016},
  issn = {0167-739X},
  author = {Shadi Ibrahim and Tien-Dat Phan and Alexandra Carpen-Amarie and Houssem-Eddine Chihoub and Diana Moise and Gabriel Antoniu},
}


@inproceedings{express-icac17,
  author={Maroulis, Stathis and Zacheilas, Nikos and Kalogeraki, Vana},
  booktitle={ICAC}, 
  title={{ExpREsS}: EneRgy Efficient Scheduling of Mixed Stream and Batch Processing Workloads}, 
  year={2017},
}

@Article{hadoop-dvfs-js17,
  author={Cai, Xiaojun and Li, Feng and Li, Ping and Ju, Lei and Jia, Zhiping},
  title={SLA-aware energy-efficient scheduling scheme for Hadoop YARN},
  journal={The Journal of Supercomputing},
  year={2017},
  month={Aug},
  day={01},
  volume={73},
  number={8},
  pages={3526-3546},
}

@article{spark-dvfs-js21,
  author={Li, Hongjian and Wei, Yaojun and Xiong, Yu and Ma, Enjie and Tian, Wenhong},
  title={A frequency-aware and energy-saving strategy based on DVFS for Spark},
  journal={The Journal of Supercomputing},
  year={2021},
  month={Oct},
  day={01},
  volume={77},
  number={10},
  pages={11575-11596},
}

@article{aienergy-joule24,
  author    = {Eric Masanet and Nuoa Lei and Jonathan Koomey},
  title     = {To better understand AIâ€™s growing energy use, analysts need a data revolution},
  journal   = {Joule},
  year      = {2024},
}

@inproceedings{dvfs-boosting-asplos24,
  author = {Piga, Leonardo and Narayanan, Iyswarya and Sundarrajan, Aditya and Skach, Matt and Deng, Qingyuan and Maity, Biswadip and Chakkaravarthy, Manoj and Huang, Alison and Dhanotia, Abhishek and Malani, Parth},
  title = {Expanding Datacenter Capacity with DVFS Boosting: A safe and scalable deployment experience},
  year = {2024},
  booktitle = {ASPLOS},
}

@inproceedings{cofris-igsc23,
  author = {Chow, Marcus and Wong, Daniel},
  title = {{CoFRIS}: Coordinated Frequency and Resource Scaling for {GPU} Inference Servers},
  year = {2024},
  booktitle = {IGSC},
}

@inproceedings{datadriven-ccgrid20,
  author={Ilager, Shashikant and Muralidhar, Rajeev and Rammohanrao, Kotagiri and Buyya, Rajkumar},
  booktitle={CCGRID}, 
  title={A Data-Driven Frequency Scaling Approach for Deadline-aware Energy Efficient Scheduling on Graphics Processing Units ({GPUs})}, 
  year={2020},
}

@inproceedings{pipefisher-mlsys23,
  author = {Osawa, Kazuki and Li, Shigang and Hoefler, Torsten},
  booktitle = {MLSys},
  title = {{PipeFisher}: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices},
  year = {2023}
}

@inproceedings{hydro-osdi23,
  author = {Qinghao Hu and Zhisheng Ye and Meng Zhang and Qiaoling Chen and Peng Sun and Yonggang Wen and Tianwei Zhang},
  title = {Hydro: {Surrogate-Based} Hyperparameter Tuning Service in Datacenters},
  booktitle = {OSDI},
  year = {2023},
}

@inproceedings{recycle-sosp24,
  author = {Swapnil Gandhi and Mark Zhao and Athinagoras Skiadopoulos and Christos Kozyrakis},
  title = {{ReCycle}: Pipeline Adaptation for the Resilient Distributed Training of Large {DNNs}},
  booktitle = {SOSP},
  year = {2024},
}

@misc{openai-keynote-hotchips24,
  title = {Predictable Scaling and Infrastructure ({HotChips} 2024 keynote talk)},
  author = {Trevor Cai},
  note = {{OpenAI}},
}

@inproceedings{crosslayer-energy-eecs24,
    author        = {Jae-Won Chung and Nishil Talati and Mosharaf Chowdhury},
    booktitle     = {Energy-Efficient Computing for Science Workshop},
    title         = {Toward Cross-Layer Energy Optimizations in {AI} Systems},
    year          = {2024},
}
